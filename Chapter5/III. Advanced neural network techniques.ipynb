{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural networks (CNNs)\n",
    "___\n",
    "One area where deep learning has achieved spectacular success is image processing. The simple classifier that we studied in detail in the previous section is severely limited – as you noticed it __wasn't even possible to classify all the smiley faces correctly__. Adding __more layers__ in the network and using __backpropagation__ to learn the weights does in principle solve the problem, but another one emerges: the __number of weights becomes extremely large__ and consequently, the __amount of training data__ required to achieve satisfactory accuracy can become __too large__ to be realistic.\n",
    "\n",
    "The __solution__ to the problem of too many weights exists: a special kind of neural network, or rather, a special kind of layer that can be included in a deep neural network, the __convolutional layer__.  Networks including convolutional layers are called __convolutional neural networks (CNNs)__.\n",
    "\n",
    "Their __key property__ is that they can detect __image features__ such as bright or dark (or specific color) spots, edges in various orientations, patterns, and so on.\n",
    "\n",
    "Normally a stop sign in the top right corner of the image would be detected only if the training data included an image with the stop sign in the top right corner. __CNNs can recognize the object anywhere__ in the image no matter where it has been observed in the training images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why we need CNNs\n",
    "___\n",
    "CNNs use a clever trick to __reduce the amount of training data__ required to detect objects in different conditions. The trick basically amounts to using the __same input weights for many neurons__ – so that all of these neurons are activated by the __same pattern – but with different input pixels__. We can for example have a set of neurons that are activated by a cat’s pointy ear. When the input is a photo of a cat, two neurons are activated, one for the left ear and another for the right. We can also let the neuron’s input pixels be taken from a smaller or a larger area, so that different neurons are activated by the ear appearing in different scales (sizes), so that we can detect a small cat's ears even if the training data only included images of big cats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mix\n",
    "___\n",
    "The __convolutional neurons__ are typically placed in the __bottom layers__ of the network, which processes the __raw input pixels__. Basic neurons (like the perceptron neuron discussed above) are placed in the higher layers, which process the output of the bottom layers. The bottom layers can usually be trained using __unsupervised learning__, without a particular prediction task in mind. Their weights will be tuned to detect features that __appear frequently__ in the input data. Thus, with photos of animals, typical features will be ears and snouts, whereas in images of buildings, the features are architectural components such as walls, roofs, windows, and so on.\n",
    "\n",
    "This means that __pre-trained convolutional layers__ can be reused in many different image processing tasks. This is extremely important since it is easy to get virtually unlimited amounts of unlabeled training data – images without labels – which can be used to train the bottom layers. The top layers are always trained by __supervised machine learning techniques__ such as __backpropagation__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative adversarial networks (GANs)\n",
    "___\n",
    "A fascinating result is obtained by taking the __pre-trained bottom layers__ and studying what the __features__ they have learned look like. This can be achieved by generating images that __activate a certain set of neurons__ in the __bottom layers__. Looking at the generated images, we can see what the neural network “thinks” __a particular feature__ looks like, or what an __image__ with a select set of features in it would look like.\n",
    "\n",
    "The idea is to let the __two networks__ compete against each other. One of the networks is trained to __generate images__ like the ones in the training data. The other network’s task is to __separate images__ generated by the first network from real images from the training data – it is called the __adversarial network__, and the whole system is called __generative adversarial network__ or a GAN.\n",
    "\n",
    "The system trains the two models side by side. In the beginning of the training, the adversarial model has an easy task to tell apart the real images from the training data and the clumsy attempts by the generative model. However, as the generative network slowly gets better and better, the adversarial model has to improve as well, and the cycle continues __until eventually the generated images are almost indistinguishable from real ones__. The GAN tries to not only reproduce the images in the training data: that would be a way too simple strategy to beat the adversarial network. Rather, the system is trained so that it has to be able to __generate new, real-looking images__ too."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
