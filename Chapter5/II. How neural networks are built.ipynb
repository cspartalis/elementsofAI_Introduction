{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and inputs\n",
    "___\n",
    "The basic artificial neuron model involves a set of **adaptive parameters**, called weights like in linear and logistic regression. Just like in regression, these weights are used as multipliers on the inputs of the neuron, which are added up. The sum of the weights times the inputs is called the **linear combination of the inputs**.\n",
    "\n",
    "**linear combination = intercept + weight1 × input1 + ... + weight6 × input6**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations and outputs\n",
    "___\n",
    "Once the linear combination has been computed, the neuron does one more operation. It takes the linear combination and puts it through a so-called activation function. Typical examples of the activation function include:\n",
    "\n",
    "* **identity function**: do nothing and just output the linear combination\n",
    "* **step function**: if the value of the linear combination is greater than zero, send a pulse (ON), otherwise do nothing (OFF)\n",
    "* **sigmoid function**: a “soft” version of the step function\n",
    "\n",
    "Note that __with the first activation function, the identity function, the neuron is exactly the same as linear regression__. This is why the identity function is rarely used in neural networks: it leads to nothing new and interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron: the mother of all ANNs\n",
    "___\n",
    "The perceptron is simply a fancy name for the simple neuron model with the **step activation function**. It was among the very first formal models of neural computation and because of its fundamental role in the history of neural networks, it wouldn’t be unfair to call it the “mother of all artificial neural networks”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers\n",
    "___\n",
    "Often the network architecture is composed of layers. The __input layer__ consists of neurons that get their inputs directly from the data. So for example, in an image recognition task, the input layer would use the pixel values of the input image as the inputs of the input layer. The network typically also has __hidden layers__ that use the other neurons´ outputs as their input, and whose output is used as the input to other layers of neurons. Finally, the __output layer__ produces the output of the whole network. All the neurons on a given layer get inputs from neurons on the previous layer and feed their output to the next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 22\n",
    "___\n",
    "<img src=\"ex22.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights example\n",
    "___\n",
    "<img src=\"weights_ex.png\"></img>"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
